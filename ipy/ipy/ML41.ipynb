{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$a_i^{(j)} = $ \"activation\" of unit $i$ in layer $j$\n",
      "\n",
      "$\\Theta^{(j)} = $ matrix of weights controlling function mapping from layer $j$ to layer $j+1$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$a_1^{(2)} = g(\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2 + \\Theta_{13}^{(1)}x_3)$$\n",
      "\n",
      "$$a_2^{(2)} = g(\\Theta_{20}^{(1)}x_0 + \\Theta_{21}^{(1)}x_1 + \\Theta_{22}^{(1)}x_2 + \\Theta_{23}^{(1)}x_3)$$\n",
      "\n",
      "$$a_3^{(2)} = g(\\Theta_{20}^{(1)}x_0 + \\Theta_{31}^{(1)}x_1 + \\Theta_{32}^{(1)}x_2 + \\Theta_{33}^{(1)}x_3)$$\n",
      "\n",
      "$$h_{\\theta}(x) = a_1^{(2)} = g(\\Theta_{10}^{(2)}a_0^{(2)} + \\Theta_{11}^{(2)}a_1^{(2)} + \\Theta_{12}^{(2)}a_2^{(2)} + \\Theta_{13}^{(2)}a_3^{(2)})$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If network has $s_j$ units in layer $j$, $s_{j+1}$ units in $j+1$, then $\\Theta^{(j)}$ will be of dimension $s_{j+1} \\times (s_j+1)$.   "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "g = lambda x: 1/(1+exp(-x))\n",
      "x = array([[1,0,0],[1,0,1],[1,1,0],[1,1,1]])\n",
      "\n",
      "h_OR = array([-200,300,300]) #returns [0,1,1,1] by means of g(h_OR.dot(x.T))\n",
      "h_AND = array([-300,200,200]) #returns [0,0,0,1]\n",
      "#h_NOR = array([200,-300,-300]) #returns [1,0,0,0]\n",
      "#h_NAND = array([300,-200,-200]) #returns [1,1,1,0]\n",
      "\n",
      "print g(h_OR.dot(concatenate((ones((1,4)),g(array([h_AND,-h_OR]).dot(x.T))),axis=0))) #XNOR returns [1,0,0,1]\n",
      "\n",
      "print g(h_AND.dot(concatenate((ones((1,4)),g(array([h_OR,-h_AND]).dot(x.T))),axis=0))) #XOR returns [0,1,1,0]\n",
      "\n",
      "#I'm not sure if the following commands are useful, but I thought I'd include them for completeness\n",
      "h_YES = array([100,100,100]) #returns [1,1,1,1] \n",
      "#h_NO = array([-100,-100,-100]) #returns [0,0,0,0] \n",
      "h_GET2 = array([-200,-200 ,300]) #returns [0,1,0,0] \n",
      "h_GET3 = array([-200,300,-200]) #returns [0,0,1,0]\n",
      "\n",
      "print g(h_AND.dot(concatenate((ones((1,4)),g(array([h_OR,-h_AND]).dot(\\\n",
      "        concatenate((ones((1,4)),g(array([h_GET3,-h_AND]).dot(x.T))),axis=0)))),axis=0)))  #returns [1,1,0,0]\n",
      "\n",
      "print g(-h_AND.dot(concatenate((ones((1,4)),g(array([h_OR,-h_AND]).dot(\\\n",
      "        concatenate((ones((1,4)),g(array([h_GET3,-h_AND]).dot(x.T))),axis=0)))),axis=0)))  #returns [0,0,1,1]\n",
      "\n",
      "print g(h_AND.dot(concatenate((ones((1,4)),g(array([h_OR,-h_AND]).dot(\\\n",
      "        concatenate((ones((1,4)),g(array([h_GET2,-h_AND]).dot(x.T))),axis=0)))),axis=0))) #returns [1,0,1,0]\n",
      "\n",
      "print g(-h_AND.dot(concatenate((ones((1,4)),g(array([h_OR,-h_AND]).dot(\\\n",
      "        concatenate((ones((1,4)),g(array([h_GET2,-h_AND]).dot(x.T))),axis=0)))),axis=0))) #returns [0,1,0,1]\n",
      "\n",
      "print g(h_AND.dot(concatenate((ones((1,4)),g(array([h_YES,-h_GET3]).dot(x.T))),axis=0))) #returns [1,1,0,1]\n",
      "\n",
      "print g(h_AND.dot(concatenate((ones((1,4)),g(array([h_YES,-h_GET2]).dot(x.T))),axis=0))) #returns [1,0,1,1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[  1.00000000e+00   1.38389653e-87   1.38389653e-87   1.00000000e+00]\n",
        "[  3.72007598e-44   1.00000000e+00   1.00000000e+00   3.72007598e-44]\n",
        "[  1.00000000e+00   1.00000000e+00   3.72007598e-44   3.72007598e-44]\n",
        "[  3.72007598e-44   3.72007598e-44   1.00000000e+00   1.00000000e+00]\n",
        "[  1.00000000e+00   3.72007598e-44   1.00000000e+00   3.72007598e-44]\n",
        "[  3.72007598e-44   1.00000000e+00   3.72007598e-44   1.00000000e+00]\n",
        "[  1.00000000e+00   1.00000000e+00   3.72007598e-44   1.00000000e+00]\n",
        "[  1.00000000e+00   3.72007598e-44   1.00000000e+00   1.00000000e+00]\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Multiclass Classification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "when A we want $h_\\Theta(x) \\approx \\begin{bmatrix}1  \\\\ 0 \\\\0\\\\0\\end{bmatrix}$ \n",
      "\n",
      "when B we want $h_\\Theta(x) \\approx \\begin{bmatrix}0 \\\\ 1 \\\\0\\\\0\\end{bmatrix}$\n",
      "\n",
      "when C we want $h_\\Theta(x) \\approx \\begin{bmatrix}0 \\\\ 0 \\\\1\\\\0\\end{bmatrix}$\n",
      "\n",
      "when D we want $h_\\Theta(x) \\approx \\begin{bmatrix}0 \\\\ 0 \\\\1\\\\0\\end{bmatrix}$\n",
      "\n",
      "Training set: $(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})$\n",
      "\n",
      "where $y^{(1)}$ is A = $\\begin{bmatrix}1\\\\0\\\\0\\\\0\\end{bmatrix}$,  etc."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Cost Function"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "L = total # of layers in network\n",
      "\n",
      "$s_l$ = # of units (not counting bias unit) in layer $l$\n",
      "\n",
      "K = # of output units/classes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$h_\\Theta(x) \\in R^K $    $(h_\\Theta(x))_i = i^{th} $ output\n",
      "$$J(\\theta) = - \\frac 1 m \\sum_{i=0}^m \\sum_{k=0}^K \\left[ y_k^{(i)} \\log(h_\\theta (x^{(i)}))_k + (1-y_k^{(i)}) \\log(1-h_\\theta (x^{(i)}))_k \\right] $$$$+\\frac \\lambda {2m}   \\sum_{l=1}^{L-1} \\sum_{j=1}^{s_{l}} \\sum_{j=1}^{s_{l+1}}  (\\Theta_{ji}^{(l)})^2 $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Forward Propagation\n",
      "\n",
      "$$a^{(1)} = x$$\n",
      "$$z^{(2)} = \\Theta^{(1)}a^{(1)}$$\n",
      "$$a^{(2)} = g(z^{(2)}) \\hspace{2mm}(add \\hspace{4mm} a_0^{(2)})$$\n",
      "$$z^{(3)} = \\Theta^{(2)}a^{(2)}$$\n",
      "$$a^{(1)} = g(z^{(3)}) \\hspace{2mm}(add \\hspace{4mm}a_0^{(3)})$$\n",
      "$$a^{(1)} = \\Theta^{(3)}a^{(3)}$$\n",
      "$$a^{(1)} = h_\\Theta(x) = g(z^{(4)})$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Backpropagation algorithm"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Training set: $\\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})\\}$\n",
      "\n",
      "Set $\\Delta_{ij}^{(l)} = 0 \\hspace 3mm (for \\hspace 2mm all \\hspace 2mm l,i,j). $\n",
      "\n",
      "For $i$ = 1 to $m$:\n",
      "    \n",
      "<p style=\"text-indent: 3em;\"> Set $a^{(1)} = x^{(i)}$</p>\n",
      "\n",
      "   <p style=\"text-indent: 3em;\"> Perform forward propagation to compute $a^{(l)}$ for $l=2,3,...,L$</p>\n",
      "\n",
      "   <p style=\"text-indent: 3em;\"> Using $y^{(i)}$, compute $\\delta^{(L)} = a^({L})-y^({i})$</p>\n",
      "\n",
      "  <p style=\"text-indent: 3em;\"> Compute $\\delta^{(L-1)},\\delta^{(L-2)},...,\\delta^{(2)}$</p>\n",
      "\n",
      "   <p style=\"text-indent: 3em;\"> $\\Delta_{ij}^{(l)} := \\Delta_{ij}^{(l)} +a_j^{(l)} \\delta_i^{(l+1)}$ </p>\n",
      "   \n",
      "$D_{ij}^{(l)} := \\frac 1 m \\Delta_{ij}^{(l)} + \\lambda \\Theta_{ij}^{(l)}$ if $j \\neq 0$\n",
      "\n",
      "$D_{ij}^{(l)} := \\frac 1 m \\Delta_{ij}^{(l)}$ if $j = 0$\n",
      "\n",
      "$\\frac{\\partial}{\\partial \\Theta_{ij}^{(l)}}J(\\Theta) = D_{ij}^{(l)}$\n",
      "    \n",
      "    \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Neuron(object):\n",
      "\n",
      "    def __init__(self):\n",
      "        self.theta = []\n",
      "        self.inputs = []\n",
      "        self.x = []\n",
      "    def give_input(self,array):\n",
      "        \"\"\"takes data like: array([[0,0,1,1],[0,1,0,1]])\"\"\"\n",
      "        self.inputs = array\n",
      "        self.x = concatenate((ones((1,array.shape[1])),array),axis=0)\n",
      "    def set_theta(self,matrix):\n",
      "        self.theta = matrix\n",
      "    def get_output(self):\n",
      "        g = lambda x: 1/(1+exp(-x))\n",
      "        return g(self.theta.dot(x.T))\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "h_OR = array([-200,300,300])    \n",
      "h_AND = array([-300,200,200])    \n",
      "n = Neuron()\n",
      "n.give_input(array([[0,0,1,1],[0,1,0,1]]))\n",
      "n.set_theta(h_AND)\n",
      "n.get_output()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 40,
       "text": [
        "array([  5.14820022e-131,   3.72007598e-044,   3.72007598e-044,\n",
        "         1.00000000e+000])"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}